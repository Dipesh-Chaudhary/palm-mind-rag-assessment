{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALLING DEPENDENCIES"
      ],
      "metadata": {
        "id": "FwcEmKdXH-zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"langchain-experimental==0.3.4\" \"langchain-google-genai==2.1.5\" \\\n",
        "\"langchain-community==0.3.25\" \"langchain==0.3.25\" \"langchain-core==0.3.65\" \\\n",
        "\"pydantic-settings==2.9.1\" \"docling==2.37.0\" \"qdrant-client\" \"langchain-qdrant\" \\\n",
        "\"langchain-text-splitters==0.3.8\" \"transformers==4.52.4\""
      ],
      "metadata": {
        "id": "1Pa73JcVEuCh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settingup Environments and Access Secrets"
      ],
      "metadata": {
        "id": "Bs0jKjztIJdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "\n",
        "# LangChain and document processing imports\n",
        "from langchain_core.documents import Document\n",
        "from docling.document_converter import DocumentConverter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_qdrant import Qdrant\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser,JsonOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Retrieve the API key from Colab's Secret Manager.\n",
        "try:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    print(\"SUCCESS: GOOGLE_API_KEY has been loaded from Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"ERROR: Could not find the secret 'GOOGLE_API_KEY'. Please add it via the ðŸ”‘ icon.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uWIVZFyFrLP",
        "outputId": "32ac152b-def1-4365-db6b-6049fe701dba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS: GOOGLE_API_KEY has been loaded from Colab Secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions the notebook will automatically ask the RAG agent."
      ],
      "metadata": {
        "id": "5glLUhalR9HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOLDEN_QUESTIONS = [\n",
        "    \"What BLEU score did the Transformer (big) model achieve on the WMT 2014 English-to-German translation task?\",\n",
        "    \"What is the main architectural component that the Transformer model uses to replace recurrence and convolutions?\",\n",
        "    \"According to the paper, what are the three primary advantages of self-attention over recurrent layers?\",\n",
        "    \"How many parallel attention layers, or 'heads', were used in the base Transformer model?\",\n",
        "    \"Does this paper present results for using the Transformer model on image classification tasks?\"\n",
        "]\n",
        "print(f\"\\nLoaded {len(GOLDEN_QUESTIONS)} golden questions for accuracy testing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1pGqKjOR6pt",
        "outputId": "a0e50620-9585-4128-d4f1-1559000fc901"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded 5 golden questions for accuracy testing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Automated \"LLM-as-a-Judge\" Evaluator Function"
      ],
      "metadata": {
        "id": "maPzz9Vtdsfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function uses a powerful LLM to automatically score the RAG agent's answers.\n",
        "\n",
        "\n",
        "class Evaluation(BaseModel):\n",
        "    score: int = Field(description=\"The score, either 0 for incorrect or 1 for correct.\")\n",
        "    reason: str = Field(description=\"A brief justification for the given score.\")\n",
        "\n",
        "async def evaluate_answers_with_llm(qa_pair_list: list):\n",
        "    \"\"\"\n",
        "    Uses a robust LLM judge with a JSON output parser to score the answers.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ¤– Initializing LLM-as-a-Judge for automated scoring...\")\n",
        "\n",
        "    # We use a powerful model for nuanced evaluation\n",
        "    judge_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
        "\n",
        "    # This parser is more robust than simple json.loads()\n",
        "    output_parser = JsonOutputParser(pydantic_object=Evaluation)\n",
        "\n",
        "    evaluation_prompt_template = \"\"\"\n",
        "    You are an impartial evaluator. Your task is to judge if the AI's answer is correct based ONLY on the provided Context.\n",
        "\n",
        "    **Context:**\n",
        "    {context}\n",
        "    ---\n",
        "    **Question:**\n",
        "    {question}\n",
        "    ---\n",
        "    **Generated Answer:**\n",
        "    {answer}\n",
        "    ---\n",
        "    Based on the context, is the Generated Answer a correct response to the Question?\n",
        "    {format_instructions}\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        template=evaluation_prompt_template,\n",
        "        partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
        "    )\n",
        "\n",
        "    evaluator_chain = prompt | judge_llm | output_parser\n",
        "\n",
        "    total_score = 0\n",
        "    for i, qa_pair in enumerate(qa_pair_list):\n",
        "        print(f\"  -> Evaluating answer {i+1}/{len(qa_pair_list)}...\")\n",
        "        try:\n",
        "            # Use ainvoke for the async call\n",
        "            result = await evaluator_chain.ainvoke(qa_pair)\n",
        "            total_score += result.get(\"score\", 0)\n",
        "        except Exception as e:\n",
        "            print(f\"    -> WARNING: Evaluation failed for one answer, scoring it 0. Error: {e}\")\n",
        "            total_score += 0 # Add 0 if parsing or anything else fails\n",
        "\n",
        "    accuracy = total_score / len(qa_pair_list)\n",
        "    print(f\"ðŸ¤– Judge finished. Final Accuracy: {accuracy:.2%}\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "57YmM4fgdp4F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The complete experiment function"
      ],
      "metadata": {
        "id": "EyGKMS97IWz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'test_document.pdf' is uploaded to the Colab session storage"
      ],
      "metadata": {
        "id": "in7zirYjIrUe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_rag_experiment(file_path, chunking_strategy, embedding_model_name):\n",
        "    \"\"\"\n",
        "    Runs a full RAG experiment and returns all metrics, including the context used for each answer.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\\n--- Running Test: [{chunking_strategy.capitalize()} + {embedding_model_name.upper()}] ---\\n{'='*30}\")\n",
        "\n",
        "    ingestion_start_time = time.time()\n",
        "    print(\"1. Loading document...\")\n",
        "    converter = DocumentConverter()\n",
        "    result = converter.convert(file_path)\n",
        "    full_text = result.document.export_to_markdown()\n",
        "    docs = [Document(page_content=full_text, metadata={\"source\": os.path.basename(file_path)})]\n",
        "\n",
        "    print(\"2. Initializing embeddings and splitter...\")\n",
        "    if embedding_model_name == \"google\":\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "    else:\n",
        "        embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", encode_kwargs={'normalize_embeddings': True})\n",
        "\n",
        "    if chunking_strategy == \"semantic\":\n",
        "        text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
        "    else:\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "    print(\"3. Chunking document...\")\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "    ingestion_latency = time.time() - ingestion_start_time\n",
        "    print(f\"   -> Ingestion complete in {ingestion_latency:.2f}s. {len(chunks)} chunks created.\")\n",
        "\n",
        "    print(\"4. Setting up in-memory RAG chain...\")\n",
        "    vectorstore = Qdrant.from_documents(chunks, embeddings, location=\":memory:\", collection_name=\"test_collection\")\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "    prompt = ChatPromptTemplate.from_template(\"Answer the question based only on the following context:\\n\\n{context}\\n\\nQuestion: {question}\")\n",
        "    rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
        "\n",
        "    print(\"5. Generating answers for accuracy testing...\")\n",
        "    qa_pairs_for_evaluation = []\n",
        "    for question in GOLDEN_QUESTIONS:\n",
        "        # First, retrieve the context for the question\n",
        "        retrieved_docs = retriever.invoke(question)\n",
        "        context_text = \"\\n---\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "        # Now, generate the answer using that specific context\n",
        "        answer = rag_chain.invoke(question)\n",
        "\n",
        "        # Store all three parts for the judge\n",
        "        qa_pairs_for_evaluation.append({\"question\": question, \"answer\": answer, \"context\": context_text})\n",
        "\n",
        "    print(\"   -> Answer generation complete.\")\n",
        "\n",
        "    return {\n",
        "        \"config_name\": f\"{chunking_strategy.capitalize()} ({embedding_model_name.upper()})\",\n",
        "        \"ingestion_latency\": f\"{ingestion_latency:.2f}s\",\n",
        "        \"chunk_count\": len(chunks),\n",
        "        \"qa_pairs\": qa_pairs_for_evaluation\n",
        "    }"
      ],
      "metadata": {
        "id": "WIcXbbSMF4Dq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executing All Experiments"
      ],
      "metadata": {
        "id": "FynajcAVJEDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "async def main():\n",
        "    TEST_DOCUMENT_PATH = \"test_document.pdf\"\n",
        "    final_report_data = []\n",
        "    if os.path.exists(TEST_DOCUMENT_PATH):\n",
        "        print(\"Running 'Fastest Baseline' configuration...\")\n",
        "        baseline_results = await run_rag_experiment(TEST_DOCUMENT_PATH, \"recursive\", \"google\")\n",
        "        print(\"\\nRunning 'Highest Quality' configuration...\")\n",
        "        quality_results = await run_rag_experiment(TEST_DOCUMENT_PATH, \"semantic\", \"google\")\n",
        "\n",
        "        if baseline_results:\n",
        "            accuracy = await evaluate_answers_with_llm(baseline_results['qa_pairs'])\n",
        "            baseline_results['accuracy'] = f\"{accuracy:.0%}\"\n",
        "            final_report_data.append(baseline_results)\n",
        "\n",
        "        if quality_results:\n",
        "            accuracy = await evaluate_answers_with_llm(quality_results['qa_pairs'])\n",
        "            quality_results['accuracy'] = f\"{accuracy:.0%}\"\n",
        "            final_report_data.append(quality_results)\n",
        "\n",
        "        print(f\"\\n\\n{'='*25} FINAL REPORT DATA {'='*25}\")\n",
        "        print(\"The final FINDINGS table is shown below :\")\n",
        "        print(\"=================================================================\\n\")\n",
        "        print(\"| Configuration | Ingestion Latency | Chunk Count | Retrieval Accuracy (Auto-Scored) |\")\n",
        "        print(\"| :--- | :--- | :--- | :--- |\")\n",
        "        for res in final_report_data:\n",
        "            print(f\"| **{res['config_name']}** | {res['ingestion_latency']} | {res['chunk_count']} | **{res['accuracy']}** |\")\n",
        "    else:\n",
        "        print(\"\\nERROR: 'test_document.pdf' not found.\")\n",
        "        print(\"Please upload the file to the Colab session storage (ðŸ“ icon).\")\n"
      ],
      "metadata": {
        "id": "G-KLYn1dF8va"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main asynchronous function\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dcdTOaYePWf",
        "outputId": "9bdbe833-2242-40be-ee30-70f02723ca34"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 'Fastest Baseline' configuration...\n",
            "\n",
            "==============================\n",
            "--- Running Test: [Recursive + GOOGLE] ---\n",
            "==============================\n",
            "1. Loading document...\n",
            "2. Initializing embeddings and splitter...\n",
            "3. Chunking document...\n",
            "   -> Ingestion complete in 35.26s. 70 chunks created.\n",
            "4. Setting up in-memory RAG chain...\n",
            "5. Generating answers for accuracy testing...\n",
            "   -> Answer generation complete.\n",
            "\n",
            "Running 'Highest Quality' configuration...\n",
            "\n",
            "==============================\n",
            "--- Running Test: [Semantic + GOOGLE] ---\n",
            "==============================\n",
            "1. Loading document...\n",
            "2. Initializing embeddings and splitter...\n",
            "3. Chunking document...\n",
            "   -> Ingestion complete in 38.81s. 21 chunks created.\n",
            "4. Setting up in-memory RAG chain...\n",
            "5. Generating answers for accuracy testing...\n",
            "   -> Answer generation complete.\n",
            "ðŸ¤– Initializing LLM-as-a-Judge for automated scoring...\n",
            "  -> Evaluating answer 1/5...\n",
            "  -> Evaluating answer 2/5...\n",
            "  -> Evaluating answer 3/5...\n",
            "  -> Evaluating answer 4/5...\n",
            "  -> Evaluating answer 5/5...\n",
            "ðŸ¤– Judge finished. Final Accuracy: 80.00%\n",
            "ðŸ¤– Initializing LLM-as-a-Judge for automated scoring...\n",
            "  -> Evaluating answer 1/5...\n",
            "  -> Evaluating answer 2/5...\n",
            "  -> Evaluating answer 3/5...\n",
            "  -> Evaluating answer 4/5...\n",
            "  -> Evaluating answer 5/5...\n",
            "ðŸ¤– Judge finished. Final Accuracy: 100.00%\n",
            "\n",
            "\n",
            "========================= FINAL REPORT DATA =========================\n",
            "The final FINDINGS table is shown below :\n",
            "=================================================================\n",
            "\n",
            "| Configuration | Ingestion Latency | Chunk Count | Retrieval Accuracy (Auto-Scored) |\n",
            "| :--- | :--- | :--- | :--- |\n",
            "| **Recursive (GOOGLE)** | 35.26s | 70 | **80%** |\n",
            "| **Semantic (GOOGLE)** | 38.81s | 21 | **100%** |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0u2bUkLlt7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
